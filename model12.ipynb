{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a248946-460f-4977-b8a2-11445e312443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "Preparing features...\n",
      "\n",
      "Training model...\n",
      "[0]\tvalidation_0-mlogloss:1.09495\n",
      "[100]\tvalidation_0-mlogloss:0.84505\n",
      "[200]\tvalidation_0-mlogloss:0.76715\n",
      "[300]\tvalidation_0-mlogloss:0.73963\n",
      "[400]\tvalidation_0-mlogloss:0.72753\n",
      "[500]\tvalidation_0-mlogloss:0.72212\n",
      "[600]\tvalidation_0-mlogloss:0.71978\n",
      "[700]\tvalidation_0-mlogloss:0.71851\n",
      "[800]\tvalidation_0-mlogloss:0.71792\n",
      "[900]\tvalidation_0-mlogloss:0.71770\n",
      "[999]\tvalidation_0-mlogloss:0.71759\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.60      0.71      0.65     13708\n",
      "        good       0.72      0.67      0.70     18278\n",
      "     neutral       0.82      0.69      0.75      9408\n",
      "\n",
      "    accuracy                           0.69     41394\n",
      "   macro avg       0.71      0.69      0.70     41394\n",
      "weighted avg       0.70      0.69      0.69     41394\n",
      "\n",
      "\n",
      "Weighted F1 Score: 0.6946\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                           feature  importance\n",
      "60        received_date_is_weekend    0.184508\n",
      "58         received_date_dayofweek    0.098834\n",
      "67         Date_released_diff_days    0.064545\n",
      "68     Date_released_diff_days_abs    0.046820\n",
      "79      payment_received_diff_days    0.045537\n",
      "93                 discount_amount    0.041688\n",
      "85    purchased_received_diff_days    0.040916\n",
      "80  payment_received_diff_days_abs    0.037236\n",
      "70    Date_estimated_diff_days_abs    0.033618\n",
      "69        Date_estimated_diff_days    0.025365\n",
      "\n",
      "Making predictions on test set...\n",
      "\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def prepare_data(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Optimized data preparation with better performance\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store all new features\n",
    "    new_features = {}\n",
    "    \n",
    "    # Remove ID columns first\n",
    "    id_columns = ['user_id', 'transaction_id', 'order_id', 'tracking_number']\n",
    "    data = df.drop(columns=id_columns, errors='ignore')\n",
    "    \n",
    "    if is_train and 'customer_experience' in data.columns:\n",
    "        data = data.drop('customer_experience', axis=1)\n",
    "    \n",
    "    # Handle numeric and categorical columns\n",
    "    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].fillna('missing')\n",
    "    \n",
    "    # Process date columns\n",
    "    date_cols = ['Date_Registered', 'payment_datetime', 'purchased_datetime', \n",
    "                 'released_date', 'estimated_delivery_date', 'received_date']\n",
    "    \n",
    "    for col in date_cols:\n",
    "        if col in data.columns:\n",
    "            dt_series = pd.to_datetime(data[col], errors='coerce')\n",
    "            \n",
    "            # Basic date features\n",
    "            new_features[f'{col}_year'] = dt_series.dt.year.fillna(-1).astype(int)\n",
    "            new_features[f'{col}_month'] = dt_series.dt.month.fillna(-1).astype(int)\n",
    "            new_features[f'{col}_day'] = dt_series.dt.day.fillna(-1).astype(int)\n",
    "            new_features[f'{col}_dayofweek'] = dt_series.dt.dayofweek.fillna(-1).astype(int)\n",
    "            new_features[f'{col}_hour'] = dt_series.dt.hour.fillna(-1).astype(int)\n",
    "            new_features[f'{col}_is_weekend'] = (dt_series.dt.dayofweek >= 5).astype(int)\n",
    "            new_features[f'{col}_is_month_end'] = dt_series.dt.is_month_end.astype(int)\n",
    "            new_features[f'{col}_quarter'] = dt_series.dt.quarter.fillna(-1).astype(int)\n",
    "    \n",
    "    # Calculate time differences\n",
    "    date_cols_present = [col for col in date_cols if col in data.columns]\n",
    "    for i in range(len(date_cols_present)):\n",
    "        for j in range(i + 1, len(date_cols_present)):\n",
    "            col1, col2 = date_cols_present[i], date_cols_present[j]\n",
    "            time_diff = (pd.to_datetime(data[col2], errors='coerce') - \n",
    "                        pd.to_datetime(data[col1], errors='coerce')).dt.total_seconds() / (24*3600)\n",
    "            diff_name = f'{col1.split(\"_\")[0]}_{col2.split(\"_\")[0]}_diff_days'\n",
    "            new_features[diff_name] = time_diff\n",
    "            new_features[f'{diff_name}_abs'] = abs(time_diff)\n",
    "    \n",
    "    # Price features\n",
    "    if 'Product_value' in data.columns and 'final_payment' in data.columns:\n",
    "        new_features['discount_amount'] = data['Product_value'] - data['final_payment']\n",
    "        new_features['discount_percentage'] = ((data['Product_value'] - data['final_payment']) / \n",
    "                                            data['Product_value'] * 100).clip(0, 100)\n",
    "        new_features['payment_ratio'] = (data['final_payment'] / data['Product_value']).clip(0, 1)\n",
    "        new_features['log_product_value'] = np.log1p(data['Product_value'])\n",
    "        new_features['log_final_payment'] = np.log1p(data['final_payment'])\n",
    "        \n",
    "        # Price tiers using qcut\n",
    "        new_features['price_tier'] = pd.qcut(data['Product_value'], \n",
    "                                           q=10, \n",
    "                                           labels=False, \n",
    "                                           duplicates='drop').astype(int)\n",
    "    \n",
    "    # Loyalty features\n",
    "    if 'loyalty_points_redeemed' in data.columns:\n",
    "        new_features['has_loyalty_points'] = (data['loyalty_points_redeemed'] > 0).astype(int)\n",
    "        new_features['log_loyalty_points'] = np.log1p(data['loyalty_points_redeemed'])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col].astype(str))\n",
    "    \n",
    "    # Drop original date columns\n",
    "    data = data.drop(columns=[col for col in date_cols if col in data.columns])\n",
    "    \n",
    "    # Add all new features at once\n",
    "    new_features_df = pd.DataFrame(new_features, index=data.index)\n",
    "    data = pd.concat([data, new_features_df], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_data = pd.read_csv('train_dataset.csv')\n",
    "test_data = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "# Prepare target variable before feature engineering\n",
    "y = train_data['customer_experience']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(\"\\nPreparing features...\")\n",
    "X_train = prepare_data(train_data, is_train=True)\n",
    "X_test = prepare_data(test_data, is_train=False)\n",
    "\n",
    "# Ensure X_train and X_test have the same columns\n",
    "common_columns = X_train.columns.intersection(X_test.columns)\n",
    "X_train = X_train[common_columns]\n",
    "X_test = X_test[common_columns]\n",
    "\n",
    "# Scale features\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.01,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'tree_method': 'hist',\n",
    "    'enable_categorical': True,\n",
    "    'early_stopping_rounds':50\n",
    "}\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "model = xgb.XGBClassifier(**params)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    \n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=le.classes_))\n",
    "\n",
    "weighted_f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "print(f\"\\nWeighted F1 Score: {weighted_f1:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).head(10))\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"\\nMaking predictions on test set...\")\n",
    "test_predictions = model.predict(X_test)\n",
    "test_predictions_labels = le.inverse_transform(test_predictions)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(test_predictions_labels)),\n",
    "    'customer_experience': test_predictions_labels\n",
    "})\n",
    "submission.to_csv('submission12.csv', index=False)\n",
    "print(\"\\nSubmission file created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
