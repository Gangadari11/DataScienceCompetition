{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151bec01-f1ac-4c57-957c-ad33166effe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "Train data shape: (206969, 26)\n",
      "Test data shape: (137971, 25)\n",
      "\n",
      "Preparing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['max_discount'] = data[discount_cols].max(axis=1)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['min_discount'] = data[discount_cols].min(axis=1)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['discount_range'] = data['max_discount'] - data['min_discount']\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['loyalty_tier_num'] = LabelEncoder().fit_transform(data['loyalty_tier'])\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['tier_price_interaction'] = data['loyalty_tier_num'] * data['Product_value']\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['min_discount'] = data[discount_cols].min(axis=1)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['discount_range'] = data['max_discount'] - data['min_discount']\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['loyalty_tier_num'] = LabelEncoder().fit_transform(data['loyalty_tier'])\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['tier_price_interaction'] = data['loyalty_tier_num'] * data['Product_value']\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[numeric_features] = X_numeric_transformed\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16504\\882526569.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f'pca_feature_{i}'] = pca_features[:, i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of features: 110\n",
      "\n",
      "Training XGBoost model with cross-validation...\n",
      "Fold 1 F1 Score: 0.6946\n",
      "Fold 2 F1 Score: 0.6931\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 203\u001b[0m\n\u001b[0;32m    200\u001b[0m y_fold_train, y_fold_val \u001b[38;5;241m=\u001b[39m y_train[train_idx], y_train[val_idx]\n\u001b[0;32m    202\u001b[0m fold_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 203\u001b[0m \u001b[43mfold_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_fold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fold_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m fold_pred \u001b[38;5;241m=\u001b[39m fold_model\u001b[38;5;241m.\u001b[39mpredict(X_fold_val)\n\u001b[0;32m    210\u001b[0m fold_score \u001b[38;5;241m=\u001b[39m f1_score(y_fold_val, fold_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Enhanced data preparation with advanced feature engineering\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Fill missing values with appropriate strategies\n",
    "    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Fill numeric columns with median\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    # Fill categorical columns with mode\n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n",
    "    \n",
    "    # Convert date columns to datetime and extract features\n",
    "    date_cols = ['Date_Registered', 'payment_datetime', 'purchased_datetime', \n",
    "                 'released_date', 'estimated_delivery_date', 'received_date']\n",
    "    \n",
    "    for col in date_cols:\n",
    "        if col in data.columns:\n",
    "            data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "            # Extract numerical features from dates\n",
    "            data[f'{col}_year'] = data[col].dt.year.fillna(-1).astype(int)\n",
    "            data[f'{col}_month'] = data[col].dt.month.fillna(-1).astype(int)\n",
    "            data[f'{col}_day'] = data[col].dt.day.fillna(-1).astype(int)\n",
    "            data[f'{col}_dayofweek'] = data[col].dt.dayofweek.fillna(-1).astype(int)\n",
    "            data[f'{col}_quarter'] = data[col].dt.quarter.fillna(-1).astype(int)\n",
    "            data[f'{col}_is_weekend'] = (data[col].dt.dayofweek >= 5).astype(int)\n",
    "            data[f'{col}_is_month_end'] = (data[col].dt.is_month_end).astype(int)\n",
    "            data[f'{col}_is_month_start'] = (data[col].dt.is_month_start).astype(int)\n",
    "            data[f'{col}_hour'] = data[col].dt.hour.fillna(-1).astype(int)\n",
    "    \n",
    "    # Calculate time differences between all date pairs\n",
    "    date_cols_dt = [col for col in date_cols if col in data.columns]\n",
    "    for i in range(len(date_cols_dt)):\n",
    "        for j in range(i + 1, len(date_cols_dt)):\n",
    "            col1, col2 = date_cols_dt[i], date_cols_dt[j]\n",
    "            diff_name = f'{col1.split(\"_\")[0]}_{col2.split(\"_\")[0]}_diff_days'\n",
    "            data[diff_name] = (data[col2] - data[col1]).dt.total_seconds() / (24*3600)\n",
    "    \n",
    "    # Drop original date columns\n",
    "    for col in date_cols:\n",
    "        if col in data.columns:\n",
    "            data = data.drop(columns=[col])\n",
    "    \n",
    "    # Create price-related features\n",
    "    if 'Product_value' in data.columns and 'final_payment' in data.columns:\n",
    "        data['price_per_loyalty_point'] = data['Product_value'] / (data['loyalty_points_redeemed'].clip(1))\n",
    "\n",
    "        data['discount_amount'] = data['Product_value'] - data['final_payment']\n",
    "        data['discount_percentage'] = (data['discount_amount'] / data['Product_value'] * 100).clip(0, 100)\n",
    "        data['price_tier'] = pd.qcut(data['Product_value'], q=10, labels=False, duplicates='drop')\n",
    "        data['price_to_loyalty_ratio'] = data['Product_value'] / (data['loyalty_points_redeemed'] + 1)\n",
    "        \n",
    "        # Log transform price features\n",
    "        data['log_product_value'] = np.log1p(data['Product_value'])\n",
    "        data['log_final_payment'] = np.log1p(data['final_payment'])\n",
    "    \n",
    "    # Enhanced loyalty features\n",
    "    if 'loyalty_points_redeemed' in data.columns:\n",
    "        data['has_redeemed_points'] = (data['loyalty_points_redeemed'] > 0).astype(int)\n",
    "        data['log_loyalty_points'] = np.log1p(data['loyalty_points_redeemed'])\n",
    "        data['points_to_value_ratio'] = data['loyalty_points_redeemed'] / (data['Product_value'] + 1)\n",
    "    \n",
    "    # Aggregate discount features\n",
    "    discount_cols = [col for col in data.columns if 'discount_percentage' in col]\n",
    "    if discount_cols:\n",
    "        data['total_discount'] = data[discount_cols].sum(axis=1)\n",
    "        data['max_discount'] = data[discount_cols].max(axis=1)\n",
    "        data['min_discount'] = data[discount_cols].min(axis=1)\n",
    "        data['discount_range'] = data['max_discount'] - data['min_discount']\n",
    "    \n",
    "    # Interaction features\n",
    "    if 'loyalty_tier' in data.columns:\n",
    "        data['loyalty_tier'] = data['loyalty_tier'].astype(str)\n",
    "        data['loyalty_tier_num'] = LabelEncoder().fit_transform(data['loyalty_tier'])\n",
    "        data['tier_price_interaction'] = data['loyalty_tier_num'] * data['Product_value']\n",
    "    \n",
    "    # Encode categorical variables with target encoding\n",
    "    categorical_cols = ['Gender', 'Is_current_loyalty_program_member', 'loyalty_tier',\n",
    "                       'payment_method', 'purchase_medium', 'shipping_method',\n",
    "                       'product_category']\n",
    "    \n",
    "    # Only encode categorical columns that exist in the dataset\n",
    "    existing_cat_cols = [col for col in categorical_cols if col in data.columns]\n",
    "    \n",
    "    # Use LabelEncoder for categorical variables\n",
    "    le_dict = {}\n",
    "    for col in existing_cat_cols:\n",
    "        le_dict[col] = LabelEncoder()\n",
    "        data[col] = le_dict[col].fit_transform(data[col].astype(str))\n",
    "        \n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_data = pd.read_csv('train_dataset.csv')\n",
    "test_data = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "print(\"\\nTrain data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "# Prepare features\n",
    "print(\"\\nPreparing features...\")\n",
    "X_train_full = prepare_data(train_data)\n",
    "X_test = prepare_data(test_data)\n",
    "\n",
    "# Remove non-feature columns from training data\n",
    "cols_to_drop = ['customer_experience', 'user_id', 'transaction_id', \n",
    "                'order_id', 'tracking_number']\n",
    "feature_cols = [col for col in X_train_full.columns \n",
    "                if col not in cols_to_drop]\n",
    "\n",
    "X = X_train_full[feature_cols]\n",
    "y = train_data['customer_experience']\n",
    "\n",
    "# Scale numerical features and apply power transform\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "power = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "X_numeric = X[numeric_features]\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "X_numeric_transformed = power.fit_transform(X_numeric_scaled)\n",
    "X[numeric_features] = X_numeric_transformed\n",
    "\n",
    "# Add PCA features\n",
    "pca = PCA(n_components=10)\n",
    "pca_features = pca.fit_transform(X_numeric_transformed)\n",
    "for i in range(pca_features.shape[1]):\n",
    "    X[f'pca_feature_{i}'] = pca_features[:, i]\n",
    "\n",
    "print(\"\\nNumber of features:\", len(X.columns))\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.15,  # changed from 0.2\n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Prepare test data\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test_numeric = X_test[numeric_features]\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric)\n",
    "X_test_numeric_transformed = power.transform(X_test_numeric_scaled)\n",
    "X_test[numeric_features] = X_test_numeric_transformed\n",
    "\n",
    "# Add PCA features to test data\n",
    "test_pca_features = pca.transform(X_test_numeric_transformed)\n",
    "for i in range(test_pca_features.shape[1]):\n",
    "    X_test[f'pca_feature_{i}'] = test_pca_features[:, i]\n",
    "\n",
    "# Optimized XGBoost parameters\n",
    "params = {\n",
    "    'n_estimators': 1000,  # keep same\n",
    "    'max_depth': 8,        # increased from 7\n",
    "    'learning_rate': 0.03,  # increased from 0.01\n",
    "    'subsample': 1,      # increased from 0.8\n",
    "    'colsample_bytree': 1,  # increased from 0.8\n",
    "    'min_child_weight': 0.5,    # decreased from 3\n",
    "    'gamma': 0.02,           # decreased from 0.1\n",
    "    'alpha': 0.02,           # decreased from 0.1\n",
    "    'lambda': 0.5,           # decreased from 1\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'tree_method': 'hist',\n",
    "    'early_stopping_rounds': 25,  # increased from 10\n",
    "    'grow_policy': 'lossguide'    # added this\n",
    "}\n",
    "\n",
    "# Initialize and train model with StratifiedKFold\n",
    "print(\"\\nTraining XGBoost model with cross-validation...\")\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    fold_model = xgb.XGBClassifier(**params)\n",
    "    fold_model.fit(\n",
    "        X_fold_train, y_fold_train,\n",
    "        eval_set=[(X_fold_val, y_fold_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    fold_pred = fold_model.predict(X_fold_val)\n",
    "    fold_score = f1_score(y_fold_val, fold_pred, average='weighted')\n",
    "    cv_scores.append(fold_score)\n",
    "    print(f\"Fold {fold} F1 Score: {fold_score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV F1 score: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores) * 2:.4f})\")\n",
    "\n",
    "# Train final model on full training data\n",
    "print(\"\\nTraining final model...\")\n",
    "final_model = xgb.XGBClassifier(**params)\n",
    "final_model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = final_model.predict(X_val)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Calculate weighted F1 score\n",
    "weighted_f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "print(f\"\\nWeighted F1 Score: {weighted_f1:.4f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).head(10))\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"\\nMaking predictions on test set...\")\n",
    "test_predictions = final_model.predict(X_test)\n",
    "test_predictions_labels = le.inverse_transform(test_predictions)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(test_predictions_labels)),\n",
    "    'customer_experience': test_predictions_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission17.csv', index=False)\n",
    "print(\"\\nSubmission file created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
