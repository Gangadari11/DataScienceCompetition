{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe34a1-8559-4422-9a88-279ebfb1e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import category_encoders as ce\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Enhanced data preparation with advanced feature engineering\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Advanced missing value handling\n",
    "    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Fill numeric columns with interpolation where possible\n",
    "    for col in numeric_cols:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            data[col] = data[col].interpolate(method='linear', limit_direction='both')\n",
    "            data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    # Advanced categorical encoding\n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].fillna('MISSING')\n",
    "        if data[col].nunique() < 10:  # For low cardinality\n",
    "            data[f'{col}_freq'] = data[col].map(data[col].value_counts(normalize=True))\n",
    "    \n",
    "    # Enhanced date feature engineering\n",
    "    date_cols = ['Date_Registered', 'payment_datetime', 'purchased_datetime', \n",
    "                 'released_date', 'estimated_delivery_date', 'received_date']\n",
    "    \n",
    "    for col in date_cols:\n",
    "        if col in data.columns:\n",
    "            data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "            # Basic date features\n",
    "            data[f'{col}_year'] = data[col].dt.year.fillna(-1).astype(int)\n",
    "            data[f'{col}_month'] = data[col].dt.month.fillna(-1).astype(int)\n",
    "            data[f'{col}_day'] = data[col].dt.day.fillna(-1).astype(int)\n",
    "            data[f'{col}_dayofweek'] = data[col].dt.dayofweek.fillna(-1).astype(int)\n",
    "            data[f'{col}_quarter'] = data[col].dt.quarter.fillna(-1).astype(int)\n",
    "            data[f'{col}_is_weekend'] = (data[col].dt.dayofweek >= 5).astype(int)\n",
    "            # Advanced date features\n",
    "            data[f'{col}_is_month_start'] = data[col].dt.is_month_start.astype(int)\n",
    "            data[f'{col}_is_month_end'] = data[col].dt.is_month_end.astype(int)\n",
    "            data[f'{col}_sin_month'] = np.sin(2 * np.pi * data[col].dt.month/12)\n",
    "            data[f'{col}_cos_month'] = np.cos(2 * np.pi * data[col].dt.month/12)\n",
    "    \n",
    "    # Advanced time difference features\n",
    "    if 'payment_datetime' in data.columns and 'purchased_datetime' in data.columns:\n",
    "        data['payment_purchase_diff'] = (data['payment_datetime'] - data['purchased_datetime']).dt.total_seconds() / 3600\n",
    "        data['payment_purchase_diff_days'] = data['payment_purchase_diff'] / 24\n",
    "        data['payment_purchase_same_day'] = (data['payment_purchase_diff_days'].abs() < 1).astype(int)\n",
    "    \n",
    "    if 'estimated_delivery_date' in data.columns and 'received_date' in data.columns:\n",
    "        data['delivery_delay'] = (data['received_date'] - data['estimated_delivery_date']).dt.total_seconds() / (24*3600)\n",
    "        data['delivery_on_time'] = (data['delivery_delay'] <= 0).astype(int)\n",
    "        data['delivery_delay_squared'] = data['delivery_delay'] ** 2\n",
    "    \n",
    "    # Drop original date columns\n",
    "    for col in date_cols:\n",
    "        if col in data.columns:\n",
    "            data = data.drop(columns=[col])\n",
    "    \n",
    "    # Enhanced price features\n",
    "    if 'Product_value' in data.columns and 'final_payment' in data.columns:\n",
    "        data['discount_amount'] = data['Product_value'] - data['final_payment']\n",
    "        data['discount_percentage'] = (data['discount_amount'] / data['Product_value'] * 100).clip(0, 100)\n",
    "        data['price_tier'] = pd.qcut(data['Product_value'], q=5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "        data['price_to_discount_ratio'] = data['Product_value'] / (data['discount_amount'] + 1)\n",
    "        data['final_payment_log'] = np.log1p(data['final_payment'])\n",
    "    \n",
    "    # Advanced loyalty features\n",
    "    if 'loyalty_points_redeemed' in data.columns:\n",
    "        data['has_redeemed_points'] = (data['loyalty_points_redeemed'] > 0).astype(int)\n",
    "        data['loyalty_points_log'] = np.log1p(data['loyalty_points_redeemed'])\n",
    "    \n",
    "    # Categorical encoding with target encoding for high cardinality\n",
    "    categorical_cols = ['Gender', 'Is_current_loyalty_program_member', 'loyalty_tier',\n",
    "                       'payment_method', 'purchase_medium', 'shipping_method',\n",
    "                       'product_category']\n",
    "    \n",
    "    existing_cat_cols = [col for col in categorical_cols if col in data.columns]\n",
    "    \n",
    "    # Use both label encoding and frequency encoding\n",
    "    le_dict = {}\n",
    "    for col in existing_cat_cols:\n",
    "        le_dict[col] = LabelEncoder()\n",
    "        data[col] = le_dict[col].fit_transform(data[col].astype(str))\n",
    "        data[f'{col}_freq'] = data[col].map(data[col].value_counts(normalize=True))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_model(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Train model with optimized parameters and learning rate scheduling\n",
    "    \"\"\"\n",
    "    # Optimized XGBoost parameters\n",
    "    params = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 7,\n",
    "        'learning_rate': 0.01,\n",
    "        'subsample': 0.85,\n",
    "        'colsample_bytree': 0.85,\n",
    "        'min_child_weight': 2,\n",
    "        'gamma': 0.05,\n",
    "        'alpha': 0.1,\n",
    "        'lambda': 1,\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': ['mlogloss', 'merror'],\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist'  # For faster training\n",
    "    }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    # Train with early stopping\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        eval_metric=['merror', 'mlogloss'],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main execution\n",
    "print(\"Loading datasets...\")\n",
    "train_data = pd.read_csv('train_dataset.csv')\n",
    "test_data = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "print(\"\\nPreparing features...\")\n",
    "X_train_full = prepare_data(train_data)\n",
    "X_test = prepare_data(test_data)\n",
    "\n",
    "# Remove non-feature columns\n",
    "cols_to_drop = ['customer_experience', 'user_id', 'transaction_id', \n",
    "                'order_id', 'tracking_number']\n",
    "feature_cols = [col for col in X_train_full.columns \n",
    "                if col not in cols_to_drop]\n",
    "\n",
    "X = X_train_full[feature_cols]\n",
    "y = train_data['customer_experience']\n",
    "\n",
    "# Use RobustScaler for better handling of outliers\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature selection using Random Forest\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selector = SelectFromModel(rf_selector, prefit=False, threshold='median')\n",
    "selector.fit(X_train, y_train)\n",
    "selected_features = X_train.columns[selector.get_support()].tolist()\n",
    "\n",
    "# Use selected features\n",
    "X_train = X_train[selected_features]\n",
    "X_val = X_val[selected_features]\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} features out of {X_scaled.shape[1]}\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "model = train_model(X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Cross-validation with stratification\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "print(f\"\\nCross-validation F1 scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1 score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Validation predictions\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).head(10))\n",
    "\n",
    "# Prepare test data with selected features\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions on test set...\")\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "test_predictions_labels = le.inverse_transform(test_predictions)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(test_predictions_labels)),\n",
    "    'customer_experience': test_predictions_labels\n",
    "})\n",
    "\n",
    "submission.to_csv('submission7.csv', index=False)\n",
    "print(\"\\nSubmission file created successfully!\")\n",
    "\n",
    "# Verify submission\n",
    "print(\"\\nVerifying saved submission file:\")\n",
    "saved_submission = pd.read_csv('submission7.csv')\n",
    "print(saved_submission.head())\n",
    "print(\"\\nShape of saved submission:\", saved_submission.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
